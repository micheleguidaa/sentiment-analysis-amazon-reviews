{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b10347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica i risultati dal file CSV\n",
    "df = pd.read_csv('../results/3class/predictions/all_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b122df",
   "metadata": {},
   "source": [
    "# Analisi Sentiment Analysis Amazon Reviews - 3 Classi\n",
    "\n",
    "Questo notebook analizza i risultati del modello di sentiment analysis a 3 classi (negativo/neutro/positivo) su recensioni Amazon.\n",
    "\n",
    "## Dataset e Modelli Analizzati\n",
    "\n",
    "Il dataset contiene predizioni di 3 diversi modelli:\n",
    "- **VADER**: Rule-based sentiment analyzer\n",
    "- **TF-IDF + Logistic Regression**: Approccio tradizionale ML\n",
    "- **Gemini**: Large Language Model di Google\n",
    "\n",
    "## Indice\n",
    "1. [Caricamento e Esplorazione dei Dati](#caricamento)\n",
    "2. [Statistiche Descrittive](#statistiche)\n",
    "3. [Analisi delle Performance](#performance)\n",
    "4. [Confronto tra Modelli](#confronto)\n",
    "5. [Analisi degli Errori](#errori)\n",
    "6. [Conclusioni](#conclusioni)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe3a4f7",
   "metadata": {},
   "source": [
    "## 1. Caricamento e Esplorazione dei Dati {#caricamento}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8786b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerie caricate con successo!\n"
     ]
    }
   ],
   "source": [
    "# Importiamo le librerie necessarie per l'analisi\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurazione per i grafici\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Librerie caricate con successo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2082f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š INFORMAZIONI GENERALI SUL DATASET\n",
      "==================================================\n",
      "Numero di campioni: 5,000\n",
      "Numero di colonne: 5\n",
      "\n",
      "Colonne disponibili:\n",
      "  - text\n",
      "  - true_label\n",
      "  - vader_pred\n",
      "  - tfidf_pred\n",
      "  - gemini_pred\n",
      "\n",
      "Prime 3 righe del dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>vader_pred</th>\n",
       "      <th>tfidf_pred</th>\n",
       "      <th>gemini_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donâ€™t waste your time!\\n\\nThese are AWFUL. The...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One Star\\n\\nI bought 4 and NONE of them worked...</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Totally useless\\n\\nOn first use it didn't heat...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text true_label vader_pred  \\\n",
       "0  Donâ€™t waste your time!\\n\\nThese are AWFUL. The...        neg        neg   \n",
       "1  One Star\\n\\nI bought 4 and NONE of them worked...        neg        pos   \n",
       "2  Totally useless\\n\\nOn first use it didn't heat...        neg        neg   \n",
       "\n",
       "  tfidf_pred gemini_pred  \n",
       "0        neg         neg  \n",
       "1        neg         neg  \n",
       "2        neg         neg  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Esploriamo il dataset\n",
    "print(\"ðŸ“Š INFORMAZIONI GENERALI SUL DATASET\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Numero di campioni: {len(df):,}\")\n",
    "print(f\"Numero di colonne: {df.shape[1]}\")\n",
    "print(f\"\\nColonne disponibili:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nPrime 3 righe del dataset:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4ac0f",
   "metadata": {},
   "source": [
    "## 2. Statistiche Descrittive {#statistiche}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb5a2073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ NORMALIZZAZIONE DEI DATI\n",
      "========================================\n",
      "Mapping completato:\n",
      "- VADER: 0 â†’ 'neg', 1 â†’ 'neu', 2 â†’ 'pos'\n",
      "- TF-IDF: 0 â†’ 'neg', 1 â†’ 'neu', 2 â†’ 'pos'\n",
      "- Gemini: giÃ  in formato stringa\n",
      "- True Labels: giÃ  in formato stringa\n",
      "\n",
      "ðŸ“ˆ DISTRIBUZIONE DELLE LABEL VERE:\n",
      "true_label\n",
      "neg    2000\n",
      "pos    2000\n",
      "neu    1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentuale:\n",
      "  neg: 40.0%\n",
      "  pos: 40.0%\n",
      "  neu: 20.0%\n"
     ]
    }
   ],
   "source": [
    "# Normalizziamo i dati per un'analisi consistente\n",
    "print(\"ðŸ”„ NORMALIZZAZIONE DEI DATI\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Convertiamo tutto a formato stringa per semplicitÃ \n",
    "df_clean = df.copy()\n",
    "\n",
    "# Mappiamo i valori numerici VADER/TF-IDF a stringhe (per 3 classi)\n",
    "# Assumendo: 0=neg, 1=neu, 2=pos per VADER e TF-IDF\n",
    "vader_mapping = {0: 'neg', 1: 'neu', 2: 'pos'}\n",
    "tfidf_mapping = {0: 'neg', 1: 'neu', 2: 'pos'}\n",
    "\n",
    "df_clean['vader_pred_str'] = df_clean['vader_pred'].map(vader_mapping)\n",
    "df_clean['tfidf_pred_str'] = df_clean['tfidf_pred'].map(tfidf_mapping)\n",
    "\n",
    "print(\"Mapping completato:\")\n",
    "print(\"- VADER: 0 â†’ 'neg', 1 â†’ 'neu', 2 â†’ 'pos'\")\n",
    "print(\"- TF-IDF: 0 â†’ 'neg', 1 â†’ 'neu', 2 â†’ 'pos'\")\n",
    "print(\"- Gemini: giÃ  in formato stringa\")\n",
    "print(\"- True Labels: giÃ  in formato stringa\")\n",
    "\n",
    "# Verifichiamo la distribuzione delle label vere\n",
    "print(\"\\nðŸ“ˆ DISTRIBUZIONE DELLE LABEL VERE:\")\n",
    "label_counts = df_clean['true_label'].value_counts()\n",
    "print(label_counts)\n",
    "print(f\"\\nPercentuale:\")\n",
    "for label, count in label_counts.items():\n",
    "    pct = (count / len(df_clean)) * 100\n",
    "    print(f\"  {label}: {pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cafe47c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m ordered_labels = [\u001b[33m'\u001b[39m\u001b[33mneg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mneu\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpos\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     19\u001b[39m ordered_counts = [counts.get(label, \u001b[32m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m ordered_labels]\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m wedges, texts, autotexts = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpie\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordered_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mordered_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautopct\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m%1.1f\u001b[39;49;00m\u001b[38;5;132;43;01m%%\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstartangle\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m90\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m ax.set_title(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m(n=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_clean)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m, fontweight=\u001b[33m'\u001b[39m\u001b[33mbold\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Miglioriamo la leggibilitÃ  del testo\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sentiment-analysis-amazon-reviews/.venv/lib/python3.13/site-packages/matplotlib/_api/deprecation.py:453\u001b[39m, in \u001b[36mmake_keyword_only.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > name_idx:\n\u001b[32m    448\u001b[39m     warn_deprecated(\n\u001b[32m    449\u001b[39m         since, message=\u001b[33m\"\u001b[39m\u001b[33mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[33m; the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mparameter will become keyword-only in \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    452\u001b[39m         name=name, obj_type=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sentiment-analysis-amazon-reviews/.venv/lib/python3.13/site-packages/matplotlib/__init__.py:1524\u001b[39m, in \u001b[36m_preprocess_data.<locals>.inner\u001b[39m\u001b[34m(ax, data, *args, **kwargs)\u001b[39m\n\u001b[32m   1521\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(ax, *args, data=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m   1523\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1524\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1525\u001b[39m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1526\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1527\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1529\u001b[39m     bound = new_sig.bind(ax, *args, **kwargs)\n\u001b[32m   1530\u001b[39m     auto_label = (bound.arguments.get(label_namer)\n\u001b[32m   1531\u001b[39m                   \u001b[38;5;129;01mor\u001b[39;00m bound.kwargs.get(label_namer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sentiment-analysis-amazon-reviews/.venv/lib/python3.13/site-packages/matplotlib/axes/_axes.py:3412\u001b[39m, in \u001b[36mAxes.pie\u001b[39m\u001b[34m(self, x, explode, labels, colors, autopct, pctdistance, shadow, labeldistance, startangle, radius, counterclock, wedgeprops, textprops, center, frame, rotatelabels, normalize, hatch)\u001b[39m\n\u001b[32m   3409\u001b[39m x += expl * math.cos(thetam)\n\u001b[32m   3410\u001b[39m y += expl * math.sin(thetam)\n\u001b[32m-> \u001b[39m\u001b[32m3412\u001b[39m w = \u001b[43mmpatches\u001b[49m\u001b[43m.\u001b[49m\u001b[43mWedge\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m360.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtheta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3413\u001b[39m \u001b[43m                   \u001b[49m\u001b[32;43m360.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtheta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3414\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_next_color\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3415\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mhatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhatch_cycle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3416\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mclip_on\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   3417\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3418\u001b[39m w.set(**wedgeprops)\n\u001b[32m   3419\u001b[39m slices.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sentiment-analysis-amazon-reviews/.venv/lib/python3.13/site-packages/matplotlib/patches.py:1252\u001b[39m, in \u001b[36mWedge.__init__\u001b[39m\u001b[34m(self, center, r, theta1, theta2, width, **kwargs)\u001b[39m\n\u001b[32m   1250\u001b[39m \u001b[38;5;28mself\u001b[39m.theta1, \u001b[38;5;28mself\u001b[39m.theta2 = theta1, theta2\n\u001b[32m   1251\u001b[39m \u001b[38;5;28mself\u001b[39m._patch_transform = transforms.IdentityTransform()\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recompute_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sentiment-analysis-amazon-reviews/.venv/lib/python3.13/site-packages/matplotlib/patches.py:1264\u001b[39m, in \u001b[36mWedge._recompute_path\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1261\u001b[39m     connector = Path.LINETO\n\u001b[32m   1263\u001b[39m \u001b[38;5;66;03m# Form the outer ring\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m arc = \u001b[43mPath\u001b[49m\u001b[43m.\u001b[49m\u001b[43marc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.width \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1267\u001b[39m     \u001b[38;5;66;03m# Partial annulus needs to draw the outer ring\u001b[39;00m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# followed by a reversed and scaled inner ring\u001b[39;00m\n\u001b[32m   1269\u001b[39m     v1 = arc.vertices\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sentiment-analysis-amazon-reviews/.venv/lib/python3.13/site-packages/matplotlib/path.py:1002\u001b[39m, in \u001b[36mPath.arc\u001b[39m\u001b[34m(cls, theta1, theta2, n, is_wedge)\u001b[39m\n\u001b[32m   1000\u001b[39m \u001b[38;5;66;03m# number of curve segments to make\u001b[39;00m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     n = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mceil\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43meta2\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43meta1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalfpi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m1\u001b[39m:\n\u001b[32m   1004\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mn must be >= 1 or None\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: cannot convert float NaN to integer"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sentiment-analysis-amazon-reviews/.venv/lib/python3.13/site-packages/IPython/core/formatters.py:402\u001b[39m, in \u001b[36mBaseFormatter.__call__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[32m    404\u001b[39m method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sentiment-analysis-amazon-reviews/.venv/lib/python3.13/site-packages/IPython/core/pylabtools.py:170\u001b[39m, in \u001b[36mprint_figure\u001b[39m\u001b[34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[39m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[32m    168\u001b[39m     FigureCanvasBase(fig)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcanvas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m data = bytes_io.getvalue()\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fmt == \u001b[33m'\u001b[39m\u001b[33msvg\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sentiment-analysis-amazon-reviews/.venv/lib/python3.13/site-packages/matplotlib/backend_bases.py:2160\u001b[39m, in \u001b[36mFigureCanvasBase.print_figure\u001b[39m\u001b[34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[39m\n\u001b[32m   2158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[32m   2159\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches == \u001b[33m\"\u001b[39m\u001b[33mtight\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2160\u001b[39m         bbox_inches = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfigure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2161\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2162\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(layout_engine, ConstrainedLayoutEngine) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m   2163\u001b[39m                 pad_inches == \u001b[33m\"\u001b[39m\u001b[33mlayout\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2164\u001b[39m             h_pad = layout_engine.get()[\u001b[33m\"\u001b[39m\u001b[33mh_pad\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sentiment-analysis-amazon-reviews/.venv/lib/python3.13/site-packages/matplotlib/figure.py:1848\u001b[39m, in \u001b[36mFigureBase.get_tightbbox\u001b[39m\u001b[34m(self, renderer, bbox_extra_artists)\u001b[39m\n\u001b[32m   1844\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ax.get_visible():\n\u001b[32m   1845\u001b[39m     \u001b[38;5;66;03m# some Axes don't take the bbox_extra_artists kwarg so we\u001b[39;00m\n\u001b[32m   1846\u001b[39m     \u001b[38;5;66;03m# need this conditional....\u001b[39;00m\n\u001b[32m   1847\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1848\u001b[39m         bbox = \u001b[43max\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1849\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1850\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1851\u001b[39m         bbox = ax.get_tightbbox(renderer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sentiment-analysis-amazon-reviews/.venv/lib/python3.13/site-packages/matplotlib/axes/_base.py:4587\u001b[39m, in \u001b[36m_AxesBase.get_tightbbox\u001b[39m\u001b[34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[39m\n\u001b[32m   4584\u001b[39m     bbox_artists = \u001b[38;5;28mself\u001b[39m.get_default_bbox_extra_artists()\n\u001b[32m   4586\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m bbox_artists:\n\u001b[32m-> \u001b[39m\u001b[32m4587\u001b[39m     bbox = \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4588\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (bbox \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4589\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[32m0\u001b[39m < bbox.width < np.inf\n\u001b[32m   4590\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[32m0\u001b[39m < bbox.height < np.inf):\n\u001b[32m   4591\u001b[39m         bb.append(bbox)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sentiment-analysis-amazon-reviews/.venv/lib/python3.13/site-packages/matplotlib/artist.py:364\u001b[39m, in \u001b[36mArtist.get_tightbbox\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_tightbbox\u001b[39m(\u001b[38;5;28mself\u001b[39m, renderer=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    349\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[33;03m    Like `.Artist.get_window_extent`, but includes any clipping.\u001b[39;00m\n\u001b[32m    351\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    362\u001b[39m \u001b[33;03m        Returns None if clipping results in no intersection.\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     bbox = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_clip_on():\n\u001b[32m    366\u001b[39m         clip_box = \u001b[38;5;28mself\u001b[39m.get_clip_box()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sentiment-analysis-amazon-reviews/.venv/lib/python3.13/site-packages/matplotlib/patches.py:655\u001b[39m, in \u001b[36mPatch.get_window_extent\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_window_extent\u001b[39m(\u001b[38;5;28mself\u001b[39m, renderer=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_extents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sentiment-analysis-amazon-reviews/.venv/lib/python3.13/site-packages/matplotlib/path.py:662\u001b[39m, in \u001b[36mPath.get_extents\u001b[39m\u001b[34m(self, transform, **kwargs)\u001b[39m\n\u001b[32m    660\u001b[39m         \u001b[38;5;66;03m# as can the ends of the curve\u001b[39;00m\n\u001b[32m    661\u001b[39m         xys.append(curve([\u001b[32m0\u001b[39m, *dzeros, \u001b[32m1\u001b[39m]))\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     xys = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(xys):\n\u001b[32m    664\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Bbox([xys.min(axis=\u001b[32m0\u001b[39m), xys.max(axis=\u001b[32m0\u001b[39m)])\n",
      "\u001b[31mValueError\u001b[39m: need at least one array to concatenate"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizziamo la distribuzione delle predizioni per ogni modello\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Distribuzione delle Predizioni per ogni Modello (3 Classi)', fontsize=16, fontweight='bold')\n",
    "\n",
    "models = [\n",
    "    ('true_label', 'True Labels', axes[0, 0]),\n",
    "    ('vader_pred_str', 'VADER', axes[0, 1]),\n",
    "    ('tfidf_pred_str', 'TF-IDF', axes[1, 0]),\n",
    "    ('gemini_pred', 'Gemini', axes[1, 1])\n",
    "]\n",
    "\n",
    "# Colori per le 3 classi\n",
    "colors = ['#FF6B6B', '#FFA726', '#4ECDC4']\n",
    "\n",
    "for col, title, ax in models:\n",
    "    counts = df_clean[col].value_counts()\n",
    "    # Ordiniamo le categorie: neg, neu, pos\n",
    "    ordered_labels = ['neg', 'neu', 'pos']\n",
    "    ordered_counts = [counts.get(label, 0) for label in ordered_labels]\n",
    "    \n",
    "    wedges, texts, autotexts = ax.pie(ordered_counts, labels=ordered_labels, autopct='%1.1f%%', \n",
    "                                     colors=colors, startangle=90)\n",
    "    ax.set_title(f'{title}\\n(n={len(df_clean):,})', fontweight='bold')\n",
    "    \n",
    "    # Miglioriamo la leggibilitÃ  del testo\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiche testuali\n",
    "print(\"\\nðŸ“Š RIEPILOGO DELLE DISTRIBUZIONI:\")\n",
    "print(\"=\"*50)\n",
    "for col, title, _ in models:\n",
    "    counts = df_clean[col].value_counts()\n",
    "    print(f\"\\n{title}:\")\n",
    "    # Ordiniamo per neg, neu, pos\n",
    "    for label in ['neg', 'neu', 'pos']:\n",
    "        if label in counts:\n",
    "            count = counts[label]\n",
    "            pct = (count / len(df_clean)) * 100\n",
    "            print(f\"  {label}: {count:,} ({pct:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"  {label}: 0 (0.0%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c1581d",
   "metadata": {},
   "source": [
    "### Osservazioni iniziali:\n",
    "- La distribuzione delle **true labels** mostra il bilanciamento del dataset tra le 3 classi\n",
    "- **VADER** potrebbe avere bias specifici per la classificazione a 3 classi\n",
    "- **TF-IDF** dovrebbe mantenere una buona rappresentazione della distribuzione originale\n",
    "- **Gemini** mostra le predizioni del modello LLM per il sentiment a 3 classi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c06943",
   "metadata": {},
   "source": [
    "## 3. Analisi delle Performance {#performance}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec0109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcoliamo le metriche di performance per ogni modello\n",
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    \"\"\"Calcola le metriche di performance per un modello\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    }\n",
    "\n",
    "# Calcoliamo le metriche per tutti i modelli\n",
    "y_true = df_clean['true_label']\n",
    "\n",
    "models_metrics = [\n",
    "    calculate_metrics(y_true, df_clean['vader_pred_str'], 'VADER'),\n",
    "    calculate_metrics(y_true, df_clean['tfidf_pred_str'], 'TF-IDF'),\n",
    "    calculate_metrics(y_true, df_clean['gemini_pred'], 'Gemini')\n",
    "]\n",
    "\n",
    "# Creiamo un DataFrame con i risultati\n",
    "metrics_df = pd.DataFrame(models_metrics)\n",
    "metrics_df = metrics_df.round(4)\n",
    "\n",
    "print(\"ðŸŽ¯ METRICHE DI PERFORMANCE (3 CLASSI)\")\n",
    "print(\"=\"*60)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Identifichiamo il modello migliore per ogni metrica\n",
    "print(\"\\nðŸ† MODELLI MIGLIORI PER METRICA:\")\n",
    "print(\"=\"*40)\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n",
    "    best_idx = metrics_df[metric].idxmax()\n",
    "    best_model = metrics_df.iloc[best_idx]['Model']\n",
    "    best_value = metrics_df.iloc[best_idx][metric]\n",
    "    print(f\"{metric}: {best_model} ({best_value:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c4d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo le metriche in un grafico a barre\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Confronto delle Metriche di Performance (3 Classi)', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    bars = ax.bar(metrics_df['Model'], metrics_df[metric], color=colors)\n",
    "    ax.set_title(f'{metric}', fontweight='bold', fontsize=14)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel('Score')\n",
    "    \n",
    "    # Aggiungiamo i valori sopra le barre\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "               f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Ruotiamo le etichette dell'asse x\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Grafico radar per confronto complessivo\n",
    "from math import pi\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "N = len(categories)\n",
    "\n",
    "# Calcola gli angoli per ogni asse\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Plot per ogni modello\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "models = ['VADER', 'TF-IDF', 'Gemini']\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    values = metrics_df[metrics_df['Model'] == model][categories].iloc[0].tolist()\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[i])\n",
    "    ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Confronto Radar delle Performance (3 Classi)', size=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice di confusione per ogni modello\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('Matrici di Confusione per ogni Modello (3 Classi)', fontsize=16, fontweight='bold')\n",
    "\n",
    "models_data = [\n",
    "    ('VADER', df_clean['vader_pred_str']),\n",
    "    ('TF-IDF', df_clean['tfidf_pred_str']),\n",
    "    ('Gemini', df_clean['gemini_pred'])\n",
    "]\n",
    "\n",
    "class_labels = ['neg', 'neu', 'pos']\n",
    "\n",
    "for i, (model_name, y_pred) in enumerate(models_data):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=class_labels)\n",
    "    \n",
    "    # Calcoliamo le percentuali\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Creiamo la heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
    "                xticklabels=[f'Pred {label}' for label in class_labels],\n",
    "                yticklabels=[f'True {label}' for label in class_labels])\n",
    "    \n",
    "    axes[i].set_title(f'{model_name}', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    # Aggiungiamo le percentuali come testo aggiuntivo\n",
    "    for j in range(3):\n",
    "        for k in range(3):\n",
    "            axes[i].text(k+0.5, j+0.7, f'({cm_percent[j,k]:.1f}%)', \n",
    "                        ha='center', va='center', fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stampiamo i report di classificazione dettagliati\n",
    "print(\"\\nðŸ“‹ REPORT DI CLASSIFICAZIONE DETTAGLIATI (3 CLASSI)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, y_pred in models_data:\n",
    "    print(f\"\\nðŸ” {model_name}:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e377078",
   "metadata": {},
   "source": [
    "## 4. Confronto tra Modelli {#confronto}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a05ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizziamo i casi di accordo e disaccordo tra i modelli\n",
    "print(\"ðŸ¤ ANALISI DEGLI ACCORDI TRA MODELLI (3 CLASSI)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Creiamo una matrice di accordo\n",
    "agreement_data = []\n",
    "\n",
    "# Confronti a coppie\n",
    "pairs = [\n",
    "    ('VADER', 'vader_pred_str', 'TF-IDF', 'tfidf_pred_str'),\n",
    "    ('VADER', 'vader_pred_str', 'Gemini', 'gemini_pred'),\n",
    "    ('TF-IDF', 'tfidf_pred_str', 'Gemini', 'gemini_pred')\n",
    "]\n",
    "\n",
    "for model1_name, model1_col, model2_name, model2_col in pairs:\n",
    "    agreement = (df_clean[model1_col] == df_clean[model2_col]).sum()\n",
    "    agreement_pct = (agreement / len(df_clean)) * 100\n",
    "    \n",
    "    agreement_data.append({\n",
    "        'Model 1': model1_name,\n",
    "        'Model 2': model2_name,\n",
    "        'Agreement': agreement,\n",
    "        'Agreement %': agreement_pct\n",
    "    })\n",
    "    \n",
    "    print(f\"{model1_name} vs {model2_name}: {agreement:,} ({agreement_pct:.1f}%)\")\n",
    "\n",
    "# Accordo tra tutti e 3 i modelli\n",
    "all_agree = ((df_clean['vader_pred_str'] == df_clean['tfidf_pred_str']) & \n",
    "             (df_clean['tfidf_pred_str'] == df_clean['gemini_pred'])).sum()\n",
    "all_agree_pct = (all_agree / len(df_clean)) * 100\n",
    "\n",
    "print(f\"\\nAccordo tra tutti e 3 i modelli: {all_agree:,} ({all_agree_pct:.1f}%)\")\n",
    "\n",
    "# Analizziamo quando tutti e 3 sono corretti\n",
    "all_correct = ((df_clean['vader_pred_str'] == df_clean['true_label']) & \n",
    "               (df_clean['tfidf_pred_str'] == df_clean['true_label']) & \n",
    "               (df_clean['gemini_pred'] == df_clean['true_label'])).sum()\n",
    "all_correct_pct = (all_correct / len(df_clean)) * 100\n",
    "\n",
    "print(f\"Tutti e 3 corretti: {all_correct:,} ({all_correct_pct:.1f}%)\")\n",
    "\n",
    "# Analizziamo quando tutti e 3 sono sbagliati\n",
    "all_wrong = ((df_clean['vader_pred_str'] != df_clean['true_label']) & \n",
    "             (df_clean['tfidf_pred_str'] != df_clean['true_label']) & \n",
    "             (df_clean['gemini_pred'] != df_clean['true_label'])).sum()\n",
    "all_wrong_pct = (all_wrong / len(df_clean)) * 100\n",
    "\n",
    "print(f\"Tutti e 3 sbagliati: {all_wrong:,} ({all_wrong_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53393157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo la matrice di accordo\n",
    "agreement_matrix = np.zeros((3, 3))\n",
    "models = ['VADER', 'TF-IDF', 'Gemini']\n",
    "model_cols = ['vader_pred_str', 'tfidf_pred_str', 'gemini_pred']\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i == j:\n",
    "            agreement_matrix[i, j] = 100  # Accordo perfetto con se stesso\n",
    "        else:\n",
    "            agreement = (df_clean[model_cols[i]] == df_clean[model_cols[j]]).sum()\n",
    "            agreement_matrix[i, j] = (agreement / len(df_clean)) * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(agreement_matrix, annot=True, fmt='.1f', cmap='RdYlBu_r', \n",
    "            xticklabels=models, yticklabels=models, ax=ax)\n",
    "ax.set_title('Matrice di Accordo tra Modelli (%)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Grafico a torta per visualizzare i pattern di accordo\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Accordi\n",
    "labels = ['Tutti e 3 d\\'accordo', 'Solo 2 d\\'accordo', 'Tutti in disaccordo']\n",
    "sizes = [all_agree_pct, 100 - all_agree_pct - (100 - all_agree_pct), 100 - all_agree_pct]\n",
    "\n",
    "# Ricalcoliamo correttamente\n",
    "only_two_agree = len(df_clean) - all_agree - ((df_clean['vader_pred_str'] != df_clean['tfidf_pred_str']) & \n",
    "                                              (df_clean['tfidf_pred_str'] != df_clean['gemini_pred']) & \n",
    "                                              (df_clean['vader_pred_str'] != df_clean['gemini_pred'])).sum()\n",
    "\n",
    "all_disagree = ((df_clean['vader_pred_str'] != df_clean['tfidf_pred_str']) & \n",
    "                (df_clean['tfidf_pred_str'] != df_clean['gemini_pred']) & \n",
    "                (df_clean['vader_pred_str'] != df_clean['gemini_pred'])).sum()\n",
    "\n",
    "sizes = [all_agree, only_two_agree, all_disagree]\n",
    "sizes_pct = [x/len(df_clean)*100 for x in sizes]\n",
    "\n",
    "colors = ['#4ECDC4', '#FFA726', '#FF6B6B']\n",
    "wedges, texts, autotexts = ax1.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax1.set_title('Pattern di Accordo tra Modelli', fontweight='bold')\n",
    "\n",
    "# Accuratezza\n",
    "correct_labels = ['Tutti e 3 corretti', 'Tutti e 3 sbagliati', 'Misto']\n",
    "correct_sizes = [all_correct, all_wrong, len(df_clean) - all_correct - all_wrong]\n",
    "correct_sizes_pct = [x/len(df_clean)*100 for x in correct_sizes]\n",
    "\n",
    "wedges, texts, autotexts = ax2.pie(correct_sizes, labels=correct_labels, autopct='%1.1f%%', \n",
    "                                  colors=colors, startangle=90)\n",
    "ax2.set_title('Accuratezza Combinata', fontweight='bold')\n",
    "\n",
    "for autotexts_list in [ax1.texts[3:], ax2.texts[3:]]:\n",
    "    for autotext in autotexts_list:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff1b1a4",
   "metadata": {},
   "source": [
    "## 5. Analisi degli Errori {#errori}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae09950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizziamo gli errori di ogni modello\n",
    "print(\"ðŸ” ANALISI DETTAGLIATA DEGLI ERRORI (3 CLASSI)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models_error_analysis = []\n",
    "\n",
    "for model_name, pred_col in [('VADER', 'vader_pred_str'), ('TF-IDF', 'tfidf_pred_str'), ('Gemini', 'gemini_pred')]:\n",
    "    errors = df_clean[df_clean[pred_col] != df_clean['true_label']]\n",
    "    error_count = len(errors)\n",
    "    error_pct = (error_count / len(df_clean)) * 100\n",
    "    \n",
    "    print(f\"\\nðŸ“Š {model_name}:\")\n",
    "    print(f\"  Errori totali: {error_count:,} ({error_pct:.1f}%)\")\n",
    "    \n",
    "    # Analisi per classe\n",
    "    error_summary = {}\n",
    "    for true_class in ['neg', 'neu', 'pos']:\n",
    "        class_errors = errors[errors['true_label'] == true_class]\n",
    "        if len(class_errors) > 0:\n",
    "            print(f\"  Errori su '{true_class}': {len(class_errors):,}\")\n",
    "            \n",
    "            # Vediamo cosa predice per questa classe\n",
    "            pred_distribution = class_errors[pred_col].value_counts()\n",
    "            for pred_class, count in pred_distribution.items():\n",
    "                pct = (count / len(class_errors)) * 100\n",
    "                print(f\"    â†’ predice '{pred_class}': {count} ({pct:.1f}%)\")\n",
    "                \n",
    "                error_summary[f\"{true_class}_to_{pred_class}\"] = {\n",
    "                    'count': count,\n",
    "                    'percentage': pct\n",
    "                }\n",
    "    \n",
    "    models_error_analysis.append({\n",
    "        'model': model_name,\n",
    "        'total_errors': error_count,\n",
    "        'error_rate': error_pct,\n",
    "        'error_patterns': error_summary\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6f7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i tassi di errore per classe\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Tassi di Errore per Classe (3 Classi)', fontsize=16, fontweight='bold')\n",
    "\n",
    "class_labels = ['neg', 'neu', 'pos']\n",
    "colors = ['#FF6B6B', '#FFA726', '#4ECDC4']\n",
    "\n",
    "for i, (model_name, pred_col) in enumerate([('VADER', 'vader_pred_str'), ('TF-IDF', 'tfidf_pred_str'), ('Gemini', 'gemini_pred')]):\n",
    "    error_rates = []\n",
    "    \n",
    "    for class_label in class_labels:\n",
    "        class_data = df_clean[df_clean['true_label'] == class_label]\n",
    "        class_errors = class_data[class_data[pred_col] != class_data['true_label']]\n",
    "        error_rate = (len(class_errors) / len(class_data)) * 100 if len(class_data) > 0 else 0\n",
    "        error_rates.append(error_rate)\n",
    "    \n",
    "    bars = axes[i].bar(class_labels, error_rates, color=colors)\n",
    "    axes[i].set_title(f'{model_name}', fontweight='bold')\n",
    "    axes[i].set_ylabel('Tasso di Errore (%)')\n",
    "    axes[i].set_ylim(0, max(max(error_rates), 50))\n",
    "    \n",
    "    # Aggiungiamo i valori sopra le barre\n",
    "    for bar, rate in zip(bars, error_rates):\n",
    "        height = bar.get_height()\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{rate:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analizziamo i pattern di errore piÃ¹ comuni\n",
    "print(\"\\nðŸŽ¯ PATTERN DI ERRORE PIÃ™ COMUNI:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Tutti i casi in cui tutti e 3 sbagliano\n",
    "all_wrong_examples = df_clean[(df_clean['vader_pred_str'] != df_clean['true_label']) & \n",
    "                              (df_clean['tfidf_pred_str'] != df_clean['true_label']) & \n",
    "                              (df_clean['gemini_pred'] != df_clean['true_label'])]\n",
    "\n",
    "print(f\"\\nCasi in cui tutti e 3 modelli sbagliano: {len(all_wrong_examples):,}\")\n",
    "\n",
    "if len(all_wrong_examples) > 0:\n",
    "    print(\"Distribuzione per classe vera:\")\n",
    "    error_by_class = all_wrong_examples['true_label'].value_counts()\n",
    "    for class_label, count in error_by_class.items():\n",
    "        pct = (count / len(all_wrong_examples)) * 100\n",
    "        print(f\"  {class_label}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nEsempi di testi piÃ¹ problematici:\")\n",
    "    sample = all_wrong_examples.head(3)\n",
    "    for idx, row in sample.iterrows():\n",
    "        print(f\"\\nâ€¢ True: {row['true_label']} | VADER: {row['vader_pred_str']} | TF-IDF: {row['tfidf_pred_str']} | Gemini: {row['gemini_pred']}\")\n",
    "        print(f\"  Text: {row['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confronto degli errori unici di ogni modello\n",
    "print(\"ðŸŽ­ ERRORI UNICI DI OGNI MODELLO:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# VADER errori unici (dove solo VADER sbaglia)\n",
    "vader_only_errors = df_clean[(df_clean['vader_pred_str'] != df_clean['true_label']) & \n",
    "                             (df_clean['tfidf_pred_str'] == df_clean['true_label']) & \n",
    "                             (df_clean['gemini_pred'] == df_clean['true_label'])]\n",
    "\n",
    "print(f\"\\nVADER errori unici: {len(vader_only_errors):,}\")\n",
    "if len(vader_only_errors) > 0:\n",
    "    pattern_counts = vader_only_errors.apply(lambda row: f\"{row['true_label']}â†’{row['vader_pred_str']}\", axis=1).value_counts()\n",
    "    for pattern, count in pattern_counts.head(3).items():\n",
    "        print(f\"  {pattern}: {count}\")\n",
    "\n",
    "# TF-IDF errori unici\n",
    "tfidf_only_errors = df_clean[(df_clean['tfidf_pred_str'] != df_clean['true_label']) & \n",
    "                             (df_clean['vader_pred_str'] == df_clean['true_label']) & \n",
    "                             (df_clean['gemini_pred'] == df_clean['true_label'])]\n",
    "\n",
    "print(f\"\\nTF-IDF errori unici: {len(tfidf_only_errors):,}\")\n",
    "if len(tfidf_only_errors) > 0:\n",
    "    pattern_counts = tfidf_only_errors.apply(lambda row: f\"{row['true_label']}â†’{row['tfidf_pred_str']}\", axis=1).value_counts()\n",
    "    for pattern, count in pattern_counts.head(3).items():\n",
    "        print(f\"  {pattern}: {count}\")\n",
    "\n",
    "# Gemini errori unici\n",
    "gemini_only_errors = df_clean[(df_clean['gemini_pred'] != df_clean['true_label']) & \n",
    "                              (df_clean['vader_pred_str'] == df_clean['true_label']) & \n",
    "                              (df_clean['tfidf_pred_str'] == df_clean['true_label'])]\n",
    "\n",
    "print(f\"\\nGemini errori unici: {len(gemini_only_errors):,}\")\n",
    "if len(gemini_only_errors) > 0:\n",
    "    pattern_counts = gemini_only_errors.apply(lambda row: f\"{row['true_label']}â†’{row['gemini_pred']}\", axis=1).value_counts()\n",
    "    for pattern, count in pattern_counts.head(3).items():\n",
    "        print(f\"  {pattern}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb79c9b9",
   "metadata": {},
   "source": [
    "## 6. Conclusioni {#conclusioni}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8595035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo un ranking finale dei modelli\n",
    "print(\"ðŸ† RANKING FINALE DEI MODELLI (3 CLASSI)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Creiamo un punteggio combinato basato su multiple metriche\n",
    "final_ranking = metrics_df.copy()\n",
    "final_ranking['Combined_Score'] = (final_ranking['Accuracy'] * 0.4 + \n",
    "                                   final_ranking['Precision'] * 0.2 + \n",
    "                                   final_ranking['Recall'] * 0.2 + \n",
    "                                   final_ranking['F1-Score'] * 0.2)\n",
    "\n",
    "final_ranking = final_ranking.sort_values('Combined_Score', ascending=False)\n",
    "\n",
    "print(\"Ranking basato su punteggio combinato:\")\n",
    "print(\"(40% Accuracy + 20% Precision + 20% Recall + 20% F1-Score)\")\n",
    "print()\n",
    "\n",
    "for i, (_, row) in enumerate(final_ranking.iterrows(), 1):\n",
    "    print(f\"{i}. {row['Model']}: {row['Combined_Score']:.4f}\")\n",
    "    print(f\"   Accuracy: {row['Accuracy']:.4f} | Precision: {row['Precision']:.4f} | Recall: {row['Recall']:.4f} | F1: {row['F1-Score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Summary finale\n",
    "print(\"ðŸ“‹ SUMMARY ESECUTIVO:\")\n",
    "print(\"=\"*30)\n",
    "print(f\"â€¢ Dataset: {len(df):,} recensioni Amazon analizzate\")\n",
    "print(f\"â€¢ Modelli confrontati: 3 (VADER, TF-IDF + Logistic Regression, Gemini)\")\n",
    "print(f\"â€¢ Classi di sentiment: 3 (negativo, neutro, positivo)\")\n",
    "print(f\"â€¢ Miglior modello overall: {final_ranking.iloc[0]['Model']}\")\n",
    "print(f\"â€¢ Accuracy massima raggiunta: {final_ranking.iloc[0]['Accuracy']:.1%}\")\n",
    "\n",
    "# Salviamo i risultati principali\n",
    "summary_data = {\n",
    "    'dataset_size': len(df),\n",
    "    'num_models': 3,\n",
    "    'num_classes': 3,\n",
    "    'best_model': final_ranking.iloc[0]['Model'],\n",
    "    'best_accuracy': final_ranking.iloc[0]['Accuracy'],\n",
    "    'all_models_metrics': final_ranking.to_dict('records'),\n",
    "    'agreement_all_models': all_agree_pct,\n",
    "    'all_correct_percentage': all_correct_pct,\n",
    "    'all_wrong_percentage': all_wrong_pct\n",
    "}\n",
    "\n",
    "# Esportiamo i risultati\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Creiamo la directory se non esiste\n",
    "os.makedirs('../results/3class', exist_ok=True)\n",
    "\n",
    "# Salviamo il summary\n",
    "with open('../results/3class/analysis_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Risultati salvati in: ../results/3class/analysis_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
